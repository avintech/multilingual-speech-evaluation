{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"昨天在公园里，一位年轻的母亲带着她的孩子在草地上野餐，因为天气很好，他们用餐布铺在地上，吃着美味的食物。\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary Richness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总词数: 36\n",
      "独特词汇数: 28\n",
      "Type-Token Ratio (TTR): 0.7777777777777778\n",
      "Root Type-Token Ratio (RTTR): 4.666666666666667\n",
      "Hapax Legomena Ratio: 0.6666666666666666\n",
      "Shannon Entropy: 4.627986806877673\n",
      "Vocabulary Richness Score: 0.7962\n",
      "\n",
      "Vocabulary Richness Percentage: 79.62\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import jieba\n",
    "\n",
    "# 分词结果\n",
    "words = jieba.lcut(text)\n",
    "\n",
    "# 总词数\n",
    "total_words = len(words)\n",
    "\n",
    "# 独特词汇数\n",
    "unique_words = set(words)\n",
    "num_unique_words = len(unique_words)\n",
    "\n",
    "# 计算Type-Token Ratio (TTR)\n",
    "ttr = num_unique_words / total_words\n",
    "\n",
    "# 计算Root Type-Token Ratio (RTTR)\n",
    "rttr = num_unique_words / np.sqrt(total_words)\n",
    "\n",
    "# 计算Hapax Legomena Ratio\n",
    "hapax_legomena = [word for word, count in Counter(words).items() if count == 1]\n",
    "hapax_legomena_ratio = len(hapax_legomena) / total_words\n",
    "\n",
    "# 计算Shannon Entropy\n",
    "frequencies = Counter(words).values()\n",
    "word_probs = [freq / total_words for freq in frequencies]\n",
    "shannon_entropy = -sum(p * np.log2(p) for p in word_probs)\n",
    "\n",
    "print(\"总词数:\", total_words)\n",
    "print(\"独特词汇数:\", num_unique_words)\n",
    "print(\"Type-Token Ratio (TTR):\", ttr)\n",
    "print(\"Root Type-Token Ratio (RTTR):\", rttr)\n",
    "print(\"Hapax Legomena Ratio:\", hapax_legomena_ratio)\n",
    "print(\"Shannon Entropy:\", shannon_entropy)\n",
    "\n",
    "# Normalize the metrics\n",
    "normalized_ttr = ttr\n",
    "normalized_rttr = rttr / np.sqrt(total_words)\n",
    "normalized_hapax = hapax_legomena_ratio\n",
    "\n",
    "# Normalize Shannon Entropy\n",
    "max_entropy = np.log2(num_unique_words) if num_unique_words > 0 else 1\n",
    "normalized_entropy = shannon_entropy / max_entropy if max_entropy > 0 else 0\n",
    "\n",
    "# Combine normalized metrics with equal weights\n",
    "vocabulary_richness_score = (\n",
    "    normalized_ttr + \n",
    "    normalized_rttr + \n",
    "    normalized_hapax + \n",
    "    normalized_entropy\n",
    ") / 4\n",
    "\n",
    "# Convert to percentage\n",
    "vocabulary_richness_percentage = vocabulary_richness_score * 100\n",
    "\n",
    "# Print final score\n",
    "print(f\"Vocabulary Richness Score: {vocabulary_richness_score:.4f}\")\n",
    "print()\n",
    "print(f\"Vocabulary Richness Percentage: {vocabulary_richness_percentage:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5W1H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "Who: 年轻的母亲和她的孩子\n",
      "What: 在草地上野餐\n",
      "Where: 公园里\n",
      "When: 昨天\n",
      "Why: 因为天气很好\n",
      "How: 他们用餐布铺在地上，吃着美味的食物。\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "\n",
    "# Initialize the model\n",
    "model = GPT4All(\"mistral-7b-instruct-v0.1.Q4_0.gguf\")\n",
    "\n",
    "# Generate 5W1H response\n",
    "prompt = (\n",
    "    text +\n",
    "    \"based on the text, identify what are the 5W1H in Chinese\"\n",
    ")\n",
    "\n",
    "# Generate response with specific parameters for consistency\n",
    "#temp=0 reduce randomness\n",
    "output = model.generate(prompt, temp=0)\n",
    "\n",
    "# Print the output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping malformed line: .\n",
      "Skipping malformed line: \n",
      "Evaluation: {'Who': 1, 'What': 1, 'Where': 1, 'When': 1, 'Why': 1, 'How': 1}\n",
      "Total Score: 6\n",
      "\n",
      "5W1H Score: 100.0\n"
     ]
    }
   ],
   "source": [
    "def parse_input(input_str):\n",
    "    # Split the input string into lines\n",
    "    lines = input_str.strip().split('\\n')\n",
    "    \n",
    "    # Create a dictionary from the lines\n",
    "    details = {}\n",
    "    for line in lines:\n",
    "        if ': ' in line:\n",
    "            key, value = line.split(': ', 1)  # Split only on the first occurrence of ': '\n",
    "            details[key] = value\n",
    "        else:\n",
    "            print(f\"Skipping malformed line: {line}\")\n",
    "    \n",
    "    return details\n",
    "\n",
    "def evaluate_details(details):\n",
    "    # Ensure details is a dictionary\n",
    "    if not isinstance(details, dict):\n",
    "        raise TypeError(\"Details should be a dictionary.\")\n",
    "    \n",
    "    # Assign 1 for known, 0 for unknown (\"不明\")\n",
    "    evaluation = {\n",
    "        \"Who\": 1 if details.get(\"Who\") not in [\"不明\", \"不知道\"] else 0,\n",
    "        \"What\": 1 if details.get(\"What\") not in [\"不明\", \"不知道\"] else 0,\n",
    "        \"Where\": 1 if details.get(\"Where\") not in [\"不明\", \"不知道\"] else 0,\n",
    "        \"When\": 1 if details.get(\"When\") not in [\"不明\", \"不知道\"] else 0,\n",
    "        \"Why\": 1 if details.get(\"Why\") not in [\"不明\", \"不知道\"] else 0,\n",
    "        \"How\": 1 if details.get(\"How\") not in [\"不明\", \"不知道\"] else 0\n",
    "    }\n",
    "    \n",
    "    # Calculate the total score\n",
    "    total_score = sum(evaluation.values())\n",
    "    \n",
    "    return evaluation, total_score\n",
    "\n",
    "# Provided input string\n",
    "input_str = output\n",
    "\n",
    "# Parse the input string into a dictionary\n",
    "details = parse_input(input_str)\n",
    "\n",
    "# Evaluate the parsed details\n",
    "evaluation, total_score = evaluate_details(details)\n",
    "\n",
    "whscore = (total_score/6)*100\n",
    "print(f\"Evaluation: {evaluation}\")\n",
    "print(f\"Total Score: {total_score}\")\n",
    "print()\n",
    "print(f\"5W1H Score: {whscore}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "#!pip install jieba transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Similarity: 0.4408\n",
      "\n",
      "Semantic Similarity Score: 44.08\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize jieba for Chinese segmentation\n",
    "def segment(text):\n",
    "    return ' '.join(jieba.cut(text))\n",
    "\n",
    "# Sample Chinese sentences\n",
    "# let sentence1 be transcribed text from student and sentence2 be reference answer\n",
    "\n",
    "sentence2 = \"\"\"\n",
    "图片中 是 家里 的 饭厅。现在 应该 是 早餐时间。爸爸、妈妈、姐姐 和 弟弟 一家人 正在 用 早餐。\n",
    "爸爸 的 早餐 是 粥/面汤，妈妈 的 是 面包、煎蛋 和 咖啡/茶，姐姐 和 弟弟 的 是 麦片 和 牛奶/鲜奶/果汁。 \n",
    "弟弟 倒 牛奶/果汁时，溢/倒  出来 了，因为 他 边 倒 边 看 平板电脑。妈妈 看了 很生气。  \n",
    "姐姐 帮 弟弟 抹/擦掉 倒/溢在 桌子上 的 牛奶/鲜奶/果汁。爸爸 看到了， \n",
    "竖起 大拇指 称赞/夸奖 姐姐。有一次，我弟弟  吃东西时 不小心 打翻 了 食物， \n",
    "我 有 帮他 清理/抹/擦 桌子。  我 认为 弟弟、妹妹 不小心 做错事时， 作为 哥哥、姐姐的，应该 帮 他们。\n",
    "\"\"\"\n",
    "\n",
    "# Segment the sentences\n",
    "segmented_sentence1 = segment(text)\n",
    "segmented_sentence2 = segment(sentence2)\n",
    "\n",
    "# Load a pre-trained Chinese model from Sentence Transformers\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Encode the segmented sentences to get their embeddings\n",
    "embedding1 = model.encode(segmented_sentence1)\n",
    "embedding2 = model.encode(segmented_sentence2)\n",
    "\n",
    "# Calculate cosine similarity between the two embeddings\n",
    "similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "similarity_score = similarity * 100\n",
    "print(f\"Semantic Similarity: {similarity:.4f}\")\n",
    "print()\n",
    "print(f\"Semantic Similarity Score: {similarity_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fluency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provided text\n",
    "text_file = \"data/reading-passage.txt\"\n",
    "recording_file = \"data/recordings/chinese/chinese_b2/0dc73844-8d4f-2b00-75f6-c6bc3d267377Text_002_Line_1.wav\"\n",
    "model = '0'\n",
    "lang = 'chinese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "树枝上有一个小鸟窝\n",
      "[ 0.00241089  0.00115967  0.00018311 ...  0.00408936  0.00030518\n",
      " -0.00262451]\n",
      "16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 18986.05frames/s]\n",
      "/home/avintech/miniconda3/lib/python3.12/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['就今下有一個小屋捏我']\n",
      "<class 'numpy.ndarray'>\n",
      "0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Fluency Score: 75\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "import modules.prepare_data as prepare_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "import librosa\n",
    "\n",
    "with open(text_file, 'r') as file:\n",
    "        provided_text = file.read()\n",
    "\n",
    "print(provided_text)\n",
    "\n",
    "#prepare the new audio and extract features\n",
    "audio_array, sampling_rate = librosa.load(recording_file, sr=None)\n",
    "audio_data = {'array': audio_array, 'sampling_rate': sampling_rate}\n",
    "\n",
    "print(audio_data['array'])\n",
    "print(audio_data['sampling_rate'])\n",
    "\n",
    "data = prepare_data.load_audio(lang,provided_text,audio_data)\n",
    "\n",
    "#use previous scaler to scale the new prediction to fit into the model\n",
    "data = pd.DataFrame([data])\n",
    "data['mfcc'] = data['mfcc'].apply(lambda x: x.flatten())\n",
    "mfcc_length = data['mfcc'].apply(len).max()\n",
    "data['mfcc'] = data['mfcc'].apply(lambda x: np.pad(x, (0, mfcc_length - len(x)), mode='constant'))\n",
    "\n",
    "# Convert mfcc column into multiple columns\n",
    "mfcc_features = np.stack(data['mfcc'].values)\n",
    "df_mfcc = pd.DataFrame(mfcc_features, index=data.index)\n",
    "X = pd.concat([data[['speech_rate', 'pause_rate', 'pronunciation_accuracy']], df_mfcc], axis=1)\n",
    "X.columns = X.columns.astype(str)\n",
    "\n",
    "#Load scalar\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.read_pickle(\"data/pickles/\"+lang+\"_X_train.pkl\")\n",
    "scaler.fit(X_train)\n",
    "\n",
    "#Normalise new data\n",
    "new_data_scaled = scaler.transform(X)\n",
    "\n",
    "# Load the model from the file\n",
    "# 0 for XGBoost, 1 for Random Forest\n",
    "print(model)\n",
    "if model == '0':\n",
    "    loaded_model = keras.models.load_model('models/model_'+lang+'.keras')\n",
    "elif model == '1':\n",
    "    loaded_model = load('models/random_forest_model.joblib')\n",
    "else:\n",
    "    exit\n",
    "\n",
    "y_pred = loaded_model.predict(new_data_scaled)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "fluency_score = int((y_pred_class[0]/4)*100)\n",
    "print(\"Fluency Score: \" + str(fluency_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Richness: 79.62\n",
      "5W1H: 100.0\n",
      "Content Relevance: 44.08\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary Richness: {vocabulary_richness_percentage:.2f}\")\n",
    "print(f\"5W1H: {whscore}\")\n",
    "print(f\"Content Relevance: {similarity_score:.2f}\")\n",
    "print(f\"Content Relevance: {similarity_score:.2f}\")\n",
    "print(f\"Content Relevance: {similarity_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
